---
description: CVPR 2020
---

# ğŸ”´ Towards End-to-end Video-based Eye-Tracking

{% file src="../.gitbook/assets/Towards End-to-end Video-based Eye-Tracking.pdf" %}

### [https://arxiv.org/abs/2007.13120](https://arxiv.org/abs/2007.13120)

### 1ã€Abstract & Intro

#### åŸºæœ¬æƒ³æ³•ï¼š

äººçœ¼è§‚å¯ŸåŒºåŸŸå¾€å¾€æ˜¯å±å¹•ä¸Šçš„æ˜¾è‘—æ€§åŒºåŸŸï¼Œè¿™å°±å¯ä»¥å½“æˆä¸€ä¸ªç‰¹å¾è¾“å…¥ç½‘ç»œã€‚

#### è´¡çŒ®ï¼š

1. æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†EVEï¼Œåˆ›é€ äº†ä¸€ä¸ªæ–°çš„baseline
2. æå‡ºä¸€ä¸ªç½‘ç»œï¼ŒGazeRefineNetï¼Œè§£å†³äº†ä»¥å¾€å¼•å…¥å±å¹•å›¾åƒå®¹æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œè¯¯å·®2.40Â°ï¼Œæ¬§æ°è·ç¦»2.75cmè¯¯å·®ï¼Œæ¯”ä¸è€ƒè™‘å±å¹•å†…å®¹é«˜äº†28%

### 2ã€Related Work

#### Remote Gaze Estimation

æœ€è¿‘ç”¨æœºå™¨å­¦ä¹ è§£å†³Gaze Estimationé—®é¢˜ï¼Œå…è®¸æ›´å¤§çš„å¤´éƒ¨å§¿æ€å˜åŒ–ï¼Œè·¨ä¸ªäººè§†çº¿ä¼°è®¡ï¼Œä¹Ÿå°±æ˜¯åœ¨æ²¡è§è¿‡çš„äººèº«ä¸Šç›´æ¥åº”ç”¨ã€‚ä¹Ÿæœ‰ç”¨è´å¶æ–¯å­¦ä¹ çš„ã€‚

æœ¬æ–‡å¼ºè°ƒäº†æ•°æ®é›†çš„é‡è¦æ€§ï¼Œæåˆ°äº†RT-GENEæ•°æ®é›†ï¼Œä½†æ˜¯ä¸æ˜¯ä¸ºå±å¹•è§†çº¿ä¼°è®¡è®¾è®¡çš„ï¼›ä¹Ÿæåˆ°äº†DynamicGazeæ•°æ®é›†ï¼Œä½†æ˜¯æ²¡å¼€æºã€‚EVEå¾ˆç‰›ï¼Œ54ä½å‚ä¸è€…ï¼Œ1230ä¸‡frameå’Œä¸€å¥—è§†è§‰åˆºæ¿€ã€‚

#### Temporal Models for Gaze Estimation

æ—¶åºæ¨¡å‹æ˜¯ä¸€ä¸ªæ–°è¯é¢˜ï¼Œæœ€å‡ºçš„å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ç”¨RNNè”åˆCNNè¿›è¡Œç‰¹å¾æå–ã€‚æ™¯è§‚å±å¹•çš„Gaze Estimationæ²¡ä»€ä¹ˆæ”¹è¿›ï¼Œä½†åœ¨EYEDIAPæ•°æ®é›†ä¸Šæ•ˆæœä»¤äººé¼“èˆã€‚ä½œè€…çš„è§†é¢‘æ•°æ®é›†åŒ…æ‹¬æ—¶é—´åŒæ­¥çš„å±å¹•è®°å½•ï¼Œç”¨æˆ·é¢å¯¹çš„æ‘„åƒå¤´ï¼Œå’Œçœ¼ç›å‡è§†æ•°æ®ã€‚

#### Refining Gaze Estimates

æœ‰çš„æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯¹ä¸ªäººè¿›è¡Œå¾®è°ƒï¼Œæ€§èƒ½æå‡ä¸é”™ï¼Œä½†æ˜¯å¾®è°ƒå°±æ˜¯é—®é¢˜ã€‚

å¯¹äºå±å¹•ï¼Œå°±å¯ä»¥è€ƒè™‘å±å¹•å†…å®¹æ˜¾è‘—æ€§ï¼Œå¹¶ä¸é¢„æµ‹äººçœ¼PoGçš„æ¨¡å‹å¯¹é½ï¼Œæé«˜å‡†ç¡®æ€§ã€‚é—®é¢˜æ˜¯å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œäºæ˜¯ä½œè€…å»ºè®®ç”¨å±å¹•å†…å®¹å’Œæœ€åˆçš„å®ç°ä¼°è®¡æ¥æ›´å‡†ç¡®åœ°æƒŠå–œè¯æœ€ç»ˆçš„å®ç°ä¼°è®¡ã€‚

### 3ã€EVE Dataset

æ¯”è¾ƒäº†EVEæ•°æ®é›†å’Œå¸¸ç”¨çš„MPIIGazeå’ŒGazeCaptureï¼Œå¾—å‡ºäº†EVEåœ¨è§†çº¿æ–¹å‘å’Œå¤´éƒ¨æ–¹å‘æœ‰ç€æ›´å¥½çš„è¡¨ç°ã€‚

### 4ã€Method

é¦–å…ˆä»‹ç»EyeNetstaticä¸å…¶é€’å½’ç‰ˆæœ¬EyeNetRNN, EyeNetLSTM, EyeNetGRUï¼Œå†ä»‹ç»GazeRefineNetã€‚

#### 4.1 EyeNet Architecture

Gazeé¢„æµ‹ç»“æœé€šè¿‡ä¸‰ç»´æ–¹å‘å‘é‡æˆ–è€…çƒåæ ‡ç³»ä¸­çš„æ¬§æ‹‰è§’è¾“å‡ºã€‚

è§’åº¦æŸå¤±å‡½æ•°ä¸º![image-20240321000103361](https://cdn.jsdelivr.net/gh/indexss/imagehost@main/img/image-20240321000103361.png)

ä¸€ä¸ªbatchä¸ºNä¸ªé•¿åº¦ä¸ºTçš„å‘é‡ã€‚

PoGæŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š![image-20240321000637283](https://cdn.jsdelivr.net/gh/indexss/imagehost@main/img/image-20240321000637283.png)&#x20;

PoGçš„é¢„æµ‹ç”¨çš„æ˜¯ResNet18ï¼Œä¸ºäº†å¼•å…¥é€’å½’ï¼Œå¯é€‰æ‹©åŠ å…¥RNNï¼ŒLSTMï¼ŒGRUå—ã€‚

#### 4.2 GazeRefineNet Architecture

ç»™å®šäººçš„å·¦å³çœ¼å›¾åƒxlå’Œxrï¼Œfä¸ºEyeNetï¼Œsl = f(xl), sr = f(xr)ï¼Œs\~ä¸ºslå’Œsrçš„ç®—æ•°å¹³å‡ã€‚

gä¸ºä¸€ä¸ªFNCï¼Œs = g(xS,s\~)ï¼Œx\_Så°±æ˜¯å±å¹•å†…å®¹ã€‚è¿™å°±æ˜¯å¼•å…¥äº†å±å¹•å›¾åƒæ¥ä¿®æ­£PoGã€‚æ²¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„åˆæ­¥PoGä¼°è®¡ s\~è¡¨ç¤ºä¸ºä¸€ä¸ªç½®ä¿¡åº¦å›¾ã€‚

ä¸ºäº†åˆ©ç”¨æ—¶åºä¿¡æ¯ï¼Œé‡‡ç”¨äº†GRU cellã€‚

Train Trickï¼šå¯é€‰åœ°åœ¨ç¼–ç å™¨å’Œè§£ç å™¨å±‚ä¹‹é—´åŠ å…¥è¿æ¥è·³è·ƒè¿æ¥ï¼Œå› ä¸ºè¿™åœ¨FCNä¸­è¢«è¯æ˜æ˜¯æœ‰å¸®åŠ©çš„ã€‚é€šè¿‡åœ¨è¾“å‡ºçƒ­å›¾ä¸Šä½¿ç”¨é€åƒç´ çš„äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±å’Œåœ¨æœ€ç»ˆæ•°å€¼ä¼°è®¡çš„PoGä¸Šä½¿ç”¨MSEæŸå¤±æ¥è®­ç»ƒGazeRefineNetï¼ŒPoGè¢«è½¬æ¢ä¸ºå˜ç±³ï¼Œä»¥é˜²æ­¢æŸå¤±é¡¹å› å…¶å¤§å°è€Œçˆ†ç‚¸ã€‚

é—®é¢˜ï¼šå¯èƒ½ä¼šå‘ç°è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„å·®å¼‚æ¯”è¾ƒå¤§ï¼Œè¿™å’Œè¶…å‚æ•°çš„è®¾ç½®ä»¥åŠè¿‡æ‹Ÿåˆæ— å…³ï¼Œé€ æˆçš„åŸå› æ˜¯ä¸ªä½“ä¹‹é—´çœ¼ç›å…‰å­¦è½´å’Œè§†è§‰è½´æœ‰ä¸€ä¸ªåç§»ï¼Œè¢«ç§°ä¸ºkappaå‚æ•°ï¼Œä¸”è™½ç„¶å…‰å­¦è½´å¯ä»¥è¢«å…‰å­¦ä»ªå™¨æµ‹é‡ï¼Œè§†è§‰è½´æ²¡æ³•è¢«æµ‹é‡ã€‚ç”±äºæ¨¡å‹æ²¡æœ‰å¯¹è¿™ä¸ªå‚æ•°çš„è®¾è®¡ï¼Œæ‰€ä»¥è¿™ä¸ªä¸œè¥¿è¢«åƒæ‰äº†ï¼Œæ‰€ä»¥å¯¹ä¸ªäººæ¥è¯´3Â°ï¼Œæ³›åŒ–5Â°ã€‚å¦‚æœå¯ä»¥æä¾›é¢å¤–ä¿¡å·ï¼ˆx\_Sï¼‰ï¼Œç½‘ç»œåº”è¯¥å¯ä»¥å…‹æœè¿™ä¸ªå›°éš¾ã€‚ä½œè€…æŠŠè¿™ä¸ªä¸œè¥¿å«**Offset Augmentation**ã€‚

<figure><img src="https://cdn.jsdelivr.net/gh/indexss/imagehost@main/img/image-20240321000807228.png" alt=""><figcaption></figcaption></figure>

### 5ã€ Results

#### 5.1 Eye Gaze Estimation

ä½œè€…ç»è¿‡æ¯”è¾ƒå‘ç°ï¼ŒEyeNetçš„GRUå’ŒLSTMå˜ä½“åœ¨è§†çº¿ä¼°è®¡è¯¯å·®å’Œç³å­”å¤§å°è¯¯å·®éƒ½æœ‰æœ€å¥½çš„è¡¨ç°ã€‚

GazeRefineNetåœ¨åŠ å…¥äº†è§†é¢‘æ˜¾è‘—æ€§ä½œä¸ºè¾“å…¥åï¼Œè§†çº¿ä¼°è®¡çš„è¯¯å·®å¾—åˆ°äº†æ”¹å–„ï¼Œå¹¶ä¸”å‘ç°åŠ å…¥Screen Contentï¼ŒOffset Augmentationï¼ŒSkip Connåï¼Œç½‘ç»œçš„æ€§èƒ½åœ¨è§†çº¿ä¼°è®¡æ–¹é¢å’ŒPoGä¼°è®¡æ–¹é¢éƒ½å­˜åœ¨è¾ƒå¤§çš„æå‡ã€‚

#### 5.2 Screen Content based Refinement of PoG

è¯„ä¼°GazeRefineNetå’Œç°æœ‰çš„åŸºäºæ˜¾è‘—æ€§çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒçš„æ—¶å€™ï¼Œä½œè€…çš„ç½‘ç»œçš„è¡¨ç°ä¹Ÿè¦ä¼˜äºä¹‹å‰æœ€å…ˆè¿›çš„UNISALæ–¹æ³•ã€‚

**å®éªŒè¿‡ç¨‹ï¼š**

5.2 Screen Content based Refinement of PoG

GazeRefineNet consists of a fully-convolutional architecture which takes as in- put a screen content frame, and an offset augmentation procedure at training time. Our baseline performance for this experiment is different to Tab. 2 as gaze errors are improved when averaging the PoG from the left and right eyes, with according adjustments to the label (averaged in screen space). Even with the new competitive baseline from PoG averaging, we find in Tab. 3 that each of our additional contributions yield large performance improvements, amounting to a 28% improvement in gaze direction error, reducing it to 2.49â—¦. While not directly comparable due to differences in setting, this value is lower even than recently reported performances of supervised few-shot adaptation approaches on in-the- wild datasets \[37,29]. Specifically, we find that the offset augmentation procedure yields the greatest performance improvements, with temporal modeling further improving performance. Skip connections between the encoder and decoder do not necessarily help (except in the case of GazeRefineNetRNN), presumably be- cause the output relies mostly on information processed at the bottleneck. We present additional experiments of GazeRefineNet in the following paragraphs, and describe their setup details in our supplementary materials.

5.2 åŸºäºå±å¹•å†…å®¹çš„PoGç»†åŒ–

GazeRefineNetç”±ä¸€ä¸ªå®Œå…¨å·ç§¯çš„æ¶æ„ç»„æˆï¼Œè¯¥æ¶æ„åœ¨è®­ç»ƒæ—¶æ¥æ”¶ä¸€ä¸ªå±å¹•å†…å®¹å¸§ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿›è¡Œåç§»å¢å¼ºè¿‡ç¨‹ã€‚æˆ‘ä»¬æ­¤å®éªŒçš„åŸºå‡†æ€§èƒ½ä¸è¡¨2ä¸åŒï¼Œå› ä¸ºå½“ä»å·¦å³çœ¼å¹³å‡PoGæ—¶ï¼Œæ³¨è§†è¯¯å·®å¾—åˆ°æ”¹å–„ï¼Œå¹¶ç›¸åº”è°ƒæ•´æ ‡ç­¾ï¼ˆåœ¨å±å¹•ç©ºé—´ä¸­å–å¹³å‡ï¼‰ã€‚å³ä½¿é‡‡ç”¨äº†æ–°çš„ç«äº‰åŸºçº¿æ¥è‡ªPoGå¹³å‡å€¼ï¼Œæˆ‘ä»¬å‘ç°åœ¨è¡¨3ä¸­ï¼Œæˆ‘ä»¬é¢å¤–è´¡çŒ®çš„æ¯ä¸€é¡¹éƒ½å¸¦æ¥äº†è¾ƒå¤§çš„æ€§èƒ½æå‡ï¼Œå°†æ³¨è§†æ–¹å‘è¯¯å·®æé«˜äº†28%ï¼Œé™ä½è‡³2.49Â°ã€‚è™½ç„¶ç”±äºè®¾ç½®ä¸Šå­˜åœ¨å·®å¼‚è€Œæ— æ³•ç›´æ¥æ¯”è¾ƒï¼Œåœ¨é‡å¤–æ•°æ®é›†ä¸Šç›‘ç£å¼å°‘æ ·æœ¬é€‚åº”æ–¹æ³•æœ€è¿‘æŠ¥å‘Šçš„æ€§èƒ½ç”šè‡³ä½äºè¿™ä¸ªæ•°å€¼\[37,29]ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å‘ç°åç§»å¢å¼ºè¿‡ç¨‹å¸¦æ¥äº†æœ€å¤§çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”æ—¶é—´å»ºæ¨¡è¿›ä¸€æ­¥æ”¹å–„äº†æ€§èƒ½ã€‚**ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´è·³è·ƒè¿æ¥å¹¶ä¸ä¸€å®šæœ‰æ‰€å¸®åŠ©ï¼ˆé™¤éæ˜¯å¯¹äºGazeRefineNetRNNï¼‰**ï¼Œå¯èƒ½æ˜¯å› ä¸ºè¾“å‡ºä¸»è¦ä¾èµ–äºåœ¨ç“¶é¢ˆå¤„å¤„ç†è¿‡çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹æ®µè½ä¸­å±•ç¤ºæ›´å¤šå…³äºGazeRefineNet çš„å®éªŒï¼Œå¹¶åœ¨è¡¥å……ææ–™ä¸­æè¿°å…¶è®¾ç½®è¯¦æƒ…ã€‚

Comparison to Saliency-based Methods. In order to assess how our GazeRefineNet approach compares with existing saliency-based methods, we im- plement two up-to-date methods loosely based on \[1] and \[48]. First, we use the state-of-the-art UNISAL approach \[12] to attain high quality visual saliency predictions. We accumulate these predictions over time for the full exposure du- ration of each visual stimulus in EVE (up to 2 minutes), which should provide the best context for alignment (as opposed to our online approach, which is lim- ited to 3 seconds of history). Standard back propagation is then used to optimize for either scale and bias in screen-space (similar to \[1]) or the visual-optical axis offset, kappa (similar to \[48]) using a KL-divergence objective between accu- mulated visual saliency predictions and accumulated heatmaps of refined gaze estimates in the screen space. Tab. 4 shows that while both saliency-based base-lines perform respectably on the well-studied image and video stimuli, they fail completely on wikipedia stimuli despite the fact that the saliency estimation model was provided with full 1080p frames (as opposed to the 128 Ã— 72 input used by GazeRefineNetGRU). Furthermore, our direct approach takes raw screen pixels and gaze estimations up to the current time-step as explicit conditions and thus is a simpler yet explicit solution for live gaze refinement that can be learned end-to-end. Both the training of our approach and its large-scale evaluation is made possible by the EVE, which should allow for insightful comparisons in the future.

**ä¸åŸºäºæ˜¾è‘—æ€§çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚**ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„GazeRefineNetæ–¹æ³•ä¸ç°æœ‰åŸºäºæ˜¾è‘—æ€§çš„æ–¹æ³•ç›¸æ¯”å¦‚ä½•ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤ç§æœ€æ–°æ–¹æ³•ï¼Œå®ƒä»¬æ¾æ•£åœ°åŸºäº\[1]å’Œ\[48]ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„UNISALæ–¹æ³•\[12]æ¥è·å–é«˜è´¨é‡çš„è§†è§‰æ˜¾è‘—æ€§é¢„æµ‹ã€‚æˆ‘ä»¬å°†è¿™äº›é¢„æµ‹éšæ—¶é—´ç´¯ç§¯ï¼Œåœ¨EVEä¸­æ¯ä¸ªè§†è§‰åˆºæ¿€çš„å®Œæ•´æ›å…‰æŒç»­æ—¶é—´å†…ï¼ˆé•¿è¾¾2åˆ†é’Ÿï¼‰ï¼Œè¿™åº”è¯¥ä¸ºå¯¹é½æä¾›æœ€ä½³èƒŒæ™¯ï¼ˆä¸æˆ‘ä»¬åœ¨çº¿æ–¹æ³•ä¸åŒï¼Œåè€…ä»…é™äº3ç§’å†å²ï¼‰ã€‚ç„¶åä½¿ç”¨æ ‡å‡†åå‘ä¼ æ’­æ¥ä¼˜åŒ–å±å¹•ç©ºé—´ä¸­å°ºåº¦å’Œåå·®ï¼ˆç±»ä¼¼äº\[1]ï¼‰æˆ–è€…è§†è§‰-å…‰è½´åç§»kappaï¼ˆç±»ä¼¼äº\[48]ï¼‰ï¼Œä½¿ç”¨ç´¯ç§¯è§†è§‰æ˜¾è‘—æ€§é¢„æµ‹å’Œåœ¨å±å¹•ç©ºé—´ä¸­ç²¾ç‚¼æ³¨è§†ä¼°è®¡çƒ­å›¾ä¹‹é—´KLæ•£åº¦ç›®æ ‡å‡½æ•°ã€‚è¡¨4æ˜¾ç¤ºï¼Œè™½ç„¶ä¸¤ç§åŸºäºæ˜¾è‘—æ€§çš„åŸºçº¿åœ¨ç»è¿‡æ·±å…¥ç ”ç©¶çš„å›¾åƒå’Œè§†é¢‘åˆºæ¿€ä¸Šè¡¨ç°å¯è§‚ï¼Œä½†å®ƒä»¬åœ¨ç»´åŸºç™¾ç§‘åˆºæ¿€ä¸Šå®Œå…¨å¤±è´¥ï¼Œå°½ç®¡ç»™å®šäº†å…¨åˆ†è¾¨ç‡1080på¸§ä½œä¸ºè¾“å…¥æ¨¡å‹ï¼ˆè€ŒéGazeRefineNetGRUæ‰€ç”¨çš„128Ã—72è¾“å…¥ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç†æ–¹å¼ä»¥å½“å‰æ—¶é—´æ­¥ä¹‹å‰åŸå§‹å±å¹•åƒç´ å’Œæ³¨è§†ä¼°è®¡ä½œä¸ºæ˜ç¡®æ¡ä»¶ï¼Œå¹¶å› æ­¤æ˜¯ä¸€ç§æ›´ç®€å•ä½†æ˜ç¡®è§£å†³æ–¹æ¡ˆç”¨äºå®æ—¶æ³¨è§†ç²¾ç‚¼å­¦ä¹ ç«¯åˆ°ç«¯ã€‚é€šè¿‡EVEä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒè¯¥æ–¹æ³•å¹¶è¿›è¡Œå¤§è§„æ¨¡è¯„ä¼°ï¼Œå¹¶ä¸”æœªæ¥åº”è¯¥å¯ä»¥è¿›è¡Œæ·±å…¥æ¯”è¾ƒã€‚

Cross-Stimuli Evaluation. We study if our method generalizes to novel stim- uli types, as this has previously been raised as in issue for saliency-based gaze alignment methods (such as in \[42]). In Tab. 5, we confirm that indeed training and testing on the same stimulus type yields the greatest improvements in gaze direction estimation (shown in diagonal of table). We find in general that large improvements can be observed even when training solely on video or wikipedia stimuli types. One assumes that this is the case due to the existence of text in our video stimuli and the existence of small amounts of images in the wikipedia stimulus. In contrast, we can see that training a model on static images only does not lead to good generalization on the stimuli types.

**äº¤å‰åˆºæ¿€è¯„ä¼°ã€‚**æˆ‘ä»¬ç ”ç©¶äº†æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¦èƒ½æ¨å¹¿åˆ°æ–°é¢–çš„åˆºæ¿€ç±»å‹ï¼Œå› ä¸ºè¿™åœ¨åŸºäºæ˜¾è‘—æ€§çš„å‡è§†å¯¹é½æ–¹æ³•ä¸­æ›¾è¢«æå‡ºè¿‡ï¼ˆä¾‹å¦‚\[42]ä¸­ï¼‰ã€‚åœ¨è¡¨5ä¸­ï¼Œæˆ‘ä»¬ç¡®è®¤ç¡®å®è®­ç»ƒå’Œæµ‹è¯•ç›¸åŒç±»å‹çš„åˆºæ¿€ä¼šä½¿å‡è§†æ–¹å‘ä¼°è®¡å¾—åˆ°æœ€å¤§æ”¹è¿›ï¼ˆæ˜¾ç¤ºåœ¨è¡¨æ ¼å¯¹è§’çº¿ä¸Šï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œå³ä½¿ä»…åœ¨è§†é¢‘æˆ–ç»´åŸºç™¾ç§‘åˆºæ¿€ç±»å‹ä¸Šè¿›è¡Œè®­ç»ƒä¹Ÿå¯ä»¥è§‚å¯Ÿåˆ°è¾ƒå¤§çš„æ”¹è¿›ã€‚**äººä»¬è®¤ä¸ºè¿™æ˜¯ç”±äºè§†é¢‘åˆºæ¿€ä¸­å­˜åœ¨æ–‡æœ¬ä»¥åŠç»´åŸºç™¾ç§‘åˆºæ¿€ä¸­å­˜åœ¨å°‘é‡å›¾åƒå¯¼è‡´çš„ã€‚**ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°**ä»…åœ¨é™æ€å›¾åƒä¸Šè®­ç»ƒæ¨¡å‹å¹¶ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿è‡³å„ç§ç±»å‹çš„åˆºæ¿€ã€‚**

Qualitative Results. We visualize our results qualitatively in Fig. 4. Specifi- cally, we can see that when provided with initial estimates of PoG over time from EyeNetGRU (far-left column), our GazeRefineNetGRU can nicely recover person-specific offsets at test time to yield improved estimates of PoG (center column). When viewed in comparison with the ground-truth (far-right column), the suc- cess of GazeRefineNetGRU in these example cases is clear. In addition, note that the final operation is not one of pure offset-correction, but that the gaze signal is more aligned with the visual layout of the screen content post-refinement.

**å®šæ€§ç»“æœã€‚**æˆ‘ä»¬åœ¨å›¾4ä¸­ä»¥å®šæ€§æ–¹å¼å±•ç¤ºäº†æˆ‘ä»¬çš„ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å½“EyeNetGRUæä¾›éšæ—¶é—´å˜åŒ–çš„PoGçš„åˆå§‹ä¼°è®¡æ—¶ï¼ˆæœ€å·¦åˆ—ï¼‰ï¼Œæˆ‘ä»¬çš„GazeRefineNetGRUèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¾ˆå¥½åœ°æ¢å¤ç‰¹å®šäºäººå‘˜çš„åç§»é‡ï¼Œä»è€Œäº§ç”Ÿæ”¹è¿›åçš„PoGä¼°è®¡å€¼ï¼ˆä¸­é—´åˆ—ï¼‰ã€‚ä¸åœ°é¢çœŸç›¸è¿›è¡Œæ¯”è¾ƒæ—¶ï¼ˆæœ€å³åˆ—ï¼‰ï¼Œåœ¨è¿™äº›ç¤ºä¾‹æƒ…å†µä¸‹ï¼ŒGazeRefineNetGRUçš„æˆåŠŸæ˜¯æ˜æ˜¾çš„ã€‚æ­¤å¤–ï¼Œè¯·æ³¨æ„æœ€ç»ˆæ“ä½œä¸ä»…ä»…æ˜¯çº¯ç²¹çš„åç§»æ ¡æ­£ï¼Œè€Œä¸”ç»è¿‡æ”¹è¿›åï¼Œå‡è§†ä¿¡å·æ›´åŠ ä¸å±å¹•å†…å®¹è§†è§‰å¸ƒå±€å¯¹é½ã€‚

\
